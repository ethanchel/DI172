{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W4D3 â€” Importing Data, Exporting Data (Exercises 1â€“6)\n",
    "\n",
    "Datasets:\n",
    "- **Exercise 3 (train.zip):** provided GitHub URL\n",
    "- **Exercise 4 (Iris_dataset.zip):** provided GitHub URL\n",
    "- **Exercise 6 (JSON):** using JSONPlaceholder users endpoint (public sample JSON API) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install useful libs (Colab/Jupyter safe)\n",
    "!pip -q install pandas openpyxl requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Exercise 1: Identifying Data Types (Structured vs Unstructured)\n",
    "\n",
    "**Answers:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A companyâ€™s financial reports stored in an Excel file â†’ **Structured**  \n",
    "2. Photographs uploaded to a social media platform â†’ **Unstructured**  \n",
    "3. A collection of news articles on a website â†’ **Unstructured** (mostly text; can be *partially structured* if you store metadata like author/date/tags)  \n",
    "4. Inventory data in a relational database â†’ **Structured**  \n",
    "5. Recorded interviews from a market research study â†’ **Unstructured**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Exercise 2: Transformation Exercise (Unstructured â†’ Structured)\n",
    "\n",
    "For each source, hereâ€™s one realistic method to convert it into structured data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Blog posts about travel experiences**  \n",
    "   - Use **NLP** to extract entities like *city/country, dates, budget, activities, sentiment*.  \n",
    "   - Store results in a table: `post_id, destination, dates, cost_estimate, key_topics, sentiment_score`.\n",
    "\n",
    "2. **Audio recordings of customer service calls**  \n",
    "   - Apply **speech-to-text** transcription, then run **topic classification** and **intent detection**.  \n",
    "   - Store: `call_id, customer_id, issue_type, resolution_status, call_duration, sentiment, keywords`.\n",
    "\n",
    "3. **Handwritten brainstorming notes**  \n",
    "   - Use **OCR** (image â†’ text), then cluster similar ideas with embeddings.  \n",
    "   - Store: `note_id, extracted_text, idea_cluster, priority, owner(optional)`.\n",
    "\n",
    "4. **A video tutorial on cooking**  \n",
    "   - Extract audio â†’ **transcribe**; optionally use **computer vision** to detect ingredients/tools.  \n",
    "   - Store: `video_id, recipe_name, ingredient_list, steps (ordered), timestamps, duration`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Exercise 3: Import A File From Kaggle (via GitHub mirror)\n",
    "\n",
    "**Goal:** download `train.zip`, unzip it, load **train.csv**, and print the first rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ZIP_URL = \"https://github.com/devtlv/Datasets-DA-Bootcamp-2-/raw/refs/heads/main/Week%204%20-%20Data%20Understanding/W4D3%20-%20Importing%20Data,%20Exporting%20D/train.zip\"\n",
    "train_zip_path = Path(\"train.zip\")\n",
    "train_extract_dir = Path(\"train_data\")\n",
    "\n",
    "# Download\n",
    "if not train_zip_path.exists():\n",
    "    !wget -q -O \"{train_zip_path}\" \"{TRAIN_ZIP_URL}\"\n",
    "    print(\"âœ… Downloaded:\", train_zip_path)\n",
    "else:\n",
    "    print(\"âœ… Already exists:\", train_zip_path)\n",
    "\n",
    "# Unzip\n",
    "train_extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "with zipfile.ZipFile(train_zip_path, \"r\") as z:\n",
    "    z.extractall(train_extract_dir)\n",
    "\n",
    "print(\"âœ… Unzipped into:\", train_extract_dir)\n",
    "\n",
    "# Find train.csv\n",
    "train_csv_candidates = list(train_extract_dir.rglob(\"train.csv\"))\n",
    "if not train_csv_candidates:\n",
    "    raise FileNotFoundError(\"train.csv not found inside the zip. Check the folder structure.\")\n",
    "train_csv_path = train_csv_candidates[0]\n",
    "train_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_csv_path)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Exercise 4: Importing A CSV File (Iris Dataset)\n",
    "\n",
    "**Goal:** download `Iris_dataset.zip`, unzip it, load the CSV, display first 5 rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS_ZIP_URL = \"https://github.com/devtlv/Datasets-DA-Bootcamp-2-/raw/refs/heads/main/Week%204%20-%20Data%20Understanding/W4D3%20-%20Importing%20Data,%20Exporting%20D/Iris_dataset.zip\"\n",
    "iris_zip_path = Path(\"Iris_dataset.zip\")\n",
    "iris_extract_dir = Path(\"iris_data\")\n",
    "\n",
    "# Download\n",
    "if not iris_zip_path.exists():\n",
    "    !wget -q -O \"{iris_zip_path}\" \"{IRIS_ZIP_URL}\"\n",
    "    print(\"âœ… Downloaded:\", iris_zip_path)\n",
    "else:\n",
    "    print(\"âœ… Already exists:\", iris_zip_path)\n",
    "\n",
    "# Unzip\n",
    "iris_extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "with zipfile.ZipFile(iris_zip_path, \"r\") as z:\n",
    "    z.extractall(iris_extract_dir)\n",
    "\n",
    "print(\"âœ… Unzipped into:\", iris_extract_dir)\n",
    "\n",
    "# Find a CSV inside\n",
    "iris_csv_candidates = list(iris_extract_dir.rglob(\"*.csv\"))\n",
    "if not iris_csv_candidates:\n",
    "    raise FileNotFoundError(\"No CSV found inside Iris_dataset.zip. Check the folder structure.\")\n",
    "iris_csv_path = iris_csv_candidates[0]\n",
    "iris_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris = pd.read_csv(iris_csv_path)\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Exercise 5: Export A DataFrame To Excel and JSON\n",
    "\n",
    "**Goal:** create a simple DataFrame, export it to:\n",
    "- Excel (`.xlsx`)\n",
    "- JSON (`.json`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_df = pd.DataFrame({\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"Age\": [25, 30, 22],\n",
    "    \"City\": [\"Paris\", \"Tel Aviv\", \"Haifa\"]\n",
    "})\n",
    "\n",
    "simple_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path = Path(\"simple_dataframe.xlsx\")\n",
    "json_path = Path(\"simple_dataframe.json\")\n",
    "\n",
    "# Export to Excel\n",
    "simple_df.to_excel(excel_path, index=False)\n",
    "\n",
    "# Export to JSON (records = list of dicts, easy to read)\n",
    "simple_df.to_json(json_path, orient=\"records\", indent=2)\n",
    "\n",
    "print(\"âœ… Exported files:\")\n",
    "print(\" -\", excel_path.resolve())\n",
    "print(\" -\", json_path.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Exercise 6: Reading JSON Data (from a URL)\n",
    "\n",
    "We use a public sample JSON endpoint from **JSONPlaceholder**:\n",
    "- `https://jsonplaceholder.typicode.com/users` \n",
    "\n",
    "**Goal:** read JSON from the URL with pandas and display first 5 rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_URL = \"https://jsonplaceholder.typicode.com/users\"\n",
    "\n",
    "df_json = pd.read_json(JSON_URL)  # pandas reads JSON directly from URL\n",
    "df_json.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
